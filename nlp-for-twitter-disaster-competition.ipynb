{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-30T08:05:41.533328Z","iopub.execute_input":"2023-05-30T08:05:41.535065Z","iopub.status.idle":"2023-05-30T08:05:41.545991Z","shell.execute_reply.started":"2023-05-30T08:05:41.535015Z","shell.execute_reply":"2023-05-30T08:05:41.544887Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Transformers and Accelerate packages should be installed or upgraded, otherwise Bert Trainer will give an error:","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:05:45.069636Z","iopub.execute_input":"2023-05-30T08:05:45.070395Z","iopub.status.idle":"2023-05-30T08:05:56.699629Z","shell.execute_reply.started":"2023-05-30T08:05:45.070361Z","shell.execute_reply":"2023-05-30T08:05:56.698502Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:02.595177Z","iopub.execute_input":"2023-05-30T08:06:02.595598Z","iopub.status.idle":"2023-05-30T08:06:02.625364Z","shell.execute_reply.started":"2023-05-30T08:06:02.595564Z","shell.execute_reply":"2023-05-30T08:06:02.624405Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset,DatasetDict\n\nds = Dataset.from_pandas(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:04.111253Z","iopub.execute_input":"2023-05-30T08:06:04.112071Z","iopub.status.idle":"2023-05-30T08:06:04.125223Z","shell.execute_reply.started":"2023-05-30T08:06:04.112032Z","shell.execute_reply":"2023-05-30T08:06:04.124280Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"I decided to use DistilBert model fine-tuned on SS2: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n\nThis model classifies the text giving the probabilities for negative and positive sentiment","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:07.029043Z","iopub.execute_input":"2023-05-30T08:06:07.029403Z","iopub.status.idle":"2023-05-30T08:06:07.034091Z","shell.execute_reply.started":"2023-05-30T08:06:07.029375Z","shell.execute_reply":"2023-05-30T08:06:07.033126Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"tokz = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:08.272486Z","iopub.execute_input":"2023-05-30T08:06:08.272910Z","iopub.status.idle":"2023-05-30T08:06:08.474889Z","shell.execute_reply.started":"2023-05-30T08:06:08.272875Z","shell.execute_reply":"2023-05-30T08:06:08.473800Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def tokenize(input):\n    return tokz(input[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:09.634542Z","iopub.execute_input":"2023-05-30T08:06:09.634938Z","iopub.status.idle":"2023-05-30T08:06:09.641051Z","shell.execute_reply.started":"2023-05-30T08:06:09.634906Z","shell.execute_reply":"2023-05-30T08:06:09.639831Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"tokenized_ds = ds.map(tokenize, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:10.783953Z","iopub.execute_input":"2023-05-30T08:06:10.784308Z","iopub.status.idle":"2023-05-30T08:06:17.637912Z","shell.execute_reply.started":"2023-05-30T08:06:10.784280Z","shell.execute_reply":"2023-05-30T08:06:17.636950Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d1a921225d4859b2e4fe122a566c52"}},"metadata":{}}]},{"cell_type":"markdown","source":"Renaming the column 'target' as transformers library expects 'labels': ","metadata":{}},{"cell_type":"code","source":"tokenized_ds = tokenized_ds.rename_columns({'target': 'labels'})","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:20.345453Z","iopub.execute_input":"2023-05-30T08:06:20.346143Z","iopub.status.idle":"2023-05-30T08:06:20.354330Z","shell.execute_reply.started":"2023-05-30T08:06:20.346108Z","shell.execute_reply":"2023-05-30T08:06:20.353301Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"dds = tokenized_ds.train_test_split(0.25, seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:23.014149Z","iopub.execute_input":"2023-05-30T08:06:23.016049Z","iopub.status.idle":"2023-05-30T08:06:23.031639Z","shell.execute_reply.started":"2023-05-30T08:06:23.016007Z","shell.execute_reply":"2023-05-30T08:06:23.030602Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 5709\n    })\n    test: Dataset({\n        features: ['id', 'keyword', 'location', 'text', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 1904\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:24.559474Z","iopub.execute_input":"2023-05-30T08:06:24.559876Z","iopub.status.idle":"2023-05-30T08:06:24.565901Z","shell.execute_reply.started":"2023-05-30T08:06:24.559843Z","shell.execute_reply":"2023-05-30T08:06:24.564880Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"The metric F1 (provided by scikit-learn) is used to evaluate the competition submissions, hence we need to create a function to process it:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    labels = eval_pred.label_ids\n    preds = eval_pred.predictions.argmax(-1)\n\n    f1 = f1_score(labels, preds, average='weighted')\n    return {\n        'f1': f1,\n    }","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:28.741501Z","iopub.execute_input":"2023-05-30T08:06:28.741896Z","iopub.status.idle":"2023-05-30T08:06:28.747550Z","shell.execute_reply.started":"2023-05-30T08:06:28.741865Z","shell.execute_reply":"2023-05-30T08:06:28.746511Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"bs = 128\nepochs = 4","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:30.946000Z","iopub.execute_input":"2023-05-30T08:06:30.946356Z","iopub.status.idle":"2023-05-30T08:06:30.950644Z","shell.execute_reply.started":"2023-05-30T08:06:30.946328Z","shell.execute_reply":"2023-05-30T08:06:30.949660Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"lr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:32.006052Z","iopub.execute_input":"2023-05-30T08:06:32.006409Z","iopub.status.idle":"2023-05-30T08:06:32.012093Z","shell.execute_reply.started":"2023-05-30T08:06:32.006380Z","shell.execute_reply":"2023-05-30T08:06:32.011135Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:34.086432Z","iopub.execute_input":"2023-05-30T08:06:34.086805Z","iopub.status.idle":"2023-05-30T08:06:34.096686Z","shell.execute_reply.started":"2023-05-30T08:06:34.086773Z","shell.execute_reply":"2023-05-30T08:06:34.095458Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:35.702664Z","iopub.execute_input":"2023-05-30T08:06:35.703076Z","iopub.status.idle":"2023-05-30T08:06:36.544415Z","shell.execute_reply.started":"2023-05-30T08:06:35.703044Z","shell.execute_reply":"2023-05-30T08:06:36.543280Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:06:39.301224Z","iopub.execute_input":"2023-05-30T08:06:39.301570Z","iopub.status.idle":"2023-05-30T08:07:13.256905Z","shell.execute_reply.started":"2023-05-30T08:06:39.301543Z","shell.execute_reply":"2023-05-30T08:07:13.255713Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [92/92 00:33, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.435363</td>\n      <td>0.826791</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.404803</td>\n      <td>0.829823</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.421728</td>\n      <td>0.832169</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.435191</td>\n      <td>0.824234</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=92, training_loss=0.3504586012467094, metrics={'train_runtime': 33.934, 'train_samples_per_second': 672.954, 'train_steps_per_second': 2.711, 'total_flos': 402753671893392.0, 'train_loss': 0.3504586012467094, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"eval_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:21.093329Z","iopub.execute_input":"2023-05-30T08:07:21.093697Z","iopub.status.idle":"2023-05-30T08:07:21.110292Z","shell.execute_reply.started":"2023-05-30T08:07:21.093666Z","shell.execute_reply":"2023-05-30T08:07:21.109458Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"eval_ds = Dataset.from_pandas(eval_df).map(tokenize, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:24.134213Z","iopub.execute_input":"2023-05-30T08:07:24.134905Z","iopub.status.idle":"2023-05-30T08:07:26.839614Z","shell.execute_reply.started":"2023-05-30T08:07:24.134863Z","shell.execute_reply":"2023-05-30T08:07:26.838491Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089597ce57ae4a8580d1992812ccfbdd"}},"metadata":{}}]},{"cell_type":"code","source":"preds = trainer.predict(eval_ds).predictions.astype(float)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:28.765495Z","iopub.execute_input":"2023-05-30T08:07:28.765908Z","iopub.status.idle":"2023-05-30T08:07:30.349085Z","shell.execute_reply.started":"2023-05-30T08:07:28.765875Z","shell.execute_reply":"2023-05-30T08:07:30.347983Z"},"trusted":true},"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"array([[-1.77832031,  1.32519531],\n       [-2.24609375,  1.83007812],\n       [-2.12890625,  1.66894531],\n       ...,\n       [-2.515625  ,  2.1171875 ],\n       [-1.77832031,  1.51660156],\n       [-2.06054688,  1.73339844]])"},"metadata":{}}]},{"cell_type":"markdown","source":"The predictions are in the format of 2D array, which contains arrays of 2 numbers, so-called \"raw scores\".\n\nWe need to transform them into probabilities, which can be done via softmax function: \n*softmax(score) = exp(score) / sum(exp(score))*\n\nThis function should be applied to each pair. As a result, we will get two numbers: negative and positive sentiment. The higher the number the higher the probability\n\nIn the cell below, probabilities is a 2D array, which contains our probabilities. Then we apply the argmax() function which gives the index of a higher value. It would mean, we will get 0 for negative (no disaster) and 1 for positive (disaster)","metadata":{}},{"cell_type":"code","source":"probabilities = np.exp(preds) / np.sum(np.exp(preds), axis=1, keepdims=True)\n\nclass_predictions = np.argmax(probabilities, axis=1)\nprobabilities","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:33.491676Z","iopub.execute_input":"2023-05-30T08:07:33.492677Z","iopub.status.idle":"2023-05-30T08:07:33.502393Z","shell.execute_reply.started":"2023-05-30T08:07:33.492634Z","shell.execute_reply":"2023-05-30T08:07:33.501180Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"array([[0.04296247, 0.95703753],\n       [0.01668906, 0.98331094],\n       [0.0219273 , 0.9780727 ],\n       ...,\n       [0.00963365, 0.99036635],\n       [0.03574581, 0.96425419],\n       [0.02201123, 0.97798877]])"},"metadata":{}}]},{"cell_type":"code","source":"eval_df['preds'] = class_predictions","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:35.946344Z","iopub.execute_input":"2023-05-30T08:07:35.946699Z","iopub.status.idle":"2023-05-30T08:07:35.953778Z","shell.execute_reply.started":"2023-05-30T08:07:35.946670Z","shell.execute_reply":"2023-05-30T08:07:35.952802Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"eval_df.drop([\"keyword\", \"location\", \"text\"],axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:37.708577Z","iopub.execute_input":"2023-05-30T08:07:37.709059Z","iopub.status.idle":"2023-05-30T08:07:37.716288Z","shell.execute_reply.started":"2023-05-30T08:07:37.709024Z","shell.execute_reply":"2023-05-30T08:07:37.715383Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"submissions_df = eval_df.rename(columns={'preds': 'target'})","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:38.991801Z","iopub.execute_input":"2023-05-30T08:07:38.992163Z","iopub.status.idle":"2023-05-30T08:07:38.999544Z","shell.execute_reply.started":"2023-05-30T08:07:38.992134Z","shell.execute_reply":"2023-05-30T08:07:38.998478Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"submissions_df.to_csv('submissions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T08:07:53.197157Z","iopub.execute_input":"2023-05-30T08:07:53.197554Z","iopub.status.idle":"2023-05-30T08:07:53.211779Z","shell.execute_reply.started":"2023-05-30T08:07:53.197525Z","shell.execute_reply":"2023-05-30T08:07:53.210823Z"},"trusted":true},"execution_count":71,"outputs":[]}]}